原创 MLSys2024 MLSys2024

_2024年07月31日 07:40_ _北京_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/iakNhheymmzBFxVjhl8Yp0gRnEAxlQ5Lfg80lMBrVKx5oOxLJOHgrbfaiawpicJIgrpskphto785RN8KI8aZZRI3w/640?wx_fmt=png&from=appmsg&wxfrom=13)

_论文原作: https://arxiv.org/pdf/2312.07104_

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/iakNhheymmzBFxVjhl8Yp0gRnEAxlQ5LfmyTgWeDvNKqvRCU9EXrTF8zcwe7ZTBAD2iad2KvUc4FkgM4e49D2KTw/640?wx_fmt=png&from=appmsg&wxfrom=13)

**摘要**

大型语言模型（LLMs）任务涉及多次生成调用、先进的提示策略、控制流程以及结构化的输入输出。然而，在编程和执行这些高级应用时，我们却面临着系统效率不足的挑战。为此，我们推出了SGLang——一个专为高效执行复杂语言模型程序而设计的系统。

SGLang由前端语言和运行时环境两大部分构成。前端部分通过引入一系列简化的编程原语，使得生成操作和平行控制的编程变得更加直观与便捷。而运行时环境，则通过一系列创新优化技术，如RadixAttention用于键值缓存的重用，以及压缩有限状态机加速结构化输出的解码过程，从而显著提升了程序的执行效率。

实验结果表明，在处理包括代理控制、逻辑推理、小样本学习基准测试、JSON解码、检索增强型生成流水线以及多轮对话等在内的各种任务时，SGLang相较于当前最先进的推理系统，展现出了高达6.4倍的性能提升，真正实现了高效执行复杂语言模型程序的目标。

**1 引言**

近年来，大型语言模型（LLMs）的能力飞跃性提升，极大地拓宽了它们的应用领域。现在，这些模型不仅能处理多样化的通用任务，还能作为自主代理独立运作。在这些高级应用中，LLMs通过多轮规划、逻辑推理以及与外部环境的互动，展现出前所未有的智能。

为了实现这些复杂功能，LLMs运用了多种工具、接收了多样化的输入模式，并采用了多种先进的提示技术，如小样本学习、自我一致性校验、思维骨架和思维树等。这些技术的结合，使得LLMs在执行任务时能够更加灵活和高效。

值得注意的是，这些高级用例往往需要多次、且通常是相互依赖的LLM调用，这标志着我们在使用LLMs时正逐渐从简单的对话交流转向更为复杂的程序化控制。我们称这些控制LLMs生成过程的程序为“语言模型程序”（LM程序）。

LM程序具有两大显著特点：首先，它们通常包含多个LLM调用，这些调用与控制流程紧密结合，共同作用于复杂任务的完成，从而提高了整体执行效率和质量；其次，LM程序能够接收并处理结构化输入，同时产生结构化的输出，这使得LM程序不仅能够独立运作，还能轻松地与其他软件系统相集成，实现更广泛的应用场景。

尽管语言模型（LM）程序的应用已经相当广泛，但当前用于构建和运行这些程序的系统效率仍然不高。我们发现了两个影响语言模型程序高效利用的主要难题：

首先，编写语言模型程序本身就是一个既繁琐又具挑战性的任务，这主要归咎于大型语言模型（LLMs）的非确定性特点。开发一个LM程序常常需要进行复杂的字符串处理、反复调整提示以进行实验调优、处理脆弱的输出解析、管理多种输入方式，并实现并行处理机制。这一系列复杂的操作，即便是对于简单的程序，也会大大降低其可读性。

其次，也是至关重要的一点，执行语言模型（LM）程序时效率低下，主要原因是计算和内存使用的冗余。尽管现有的顶尖推理引擎（如vLLM、TGI和TensorRT-LLM）已经通过优化减少了延迟并提高了吞吐量，但它们并不针对具体的工作负载进行优化。这种通用性和鲁棒性虽好，却也让它们在处理特定任务时显得力不从心。

**一个显著的例子是键值（KV）缓存的重用问题**：KV缓存中存储了生成推理过程中至关重要的可重用中间数据。在LM程序的批量执行过程中，如果有多个大型语言模型（LLM）调用共享了相同的前缀，那么理论上可以多次重用KV缓存中的数据。然而，现有的系统却缺乏有效的机制来支持这种重用，结果导致了大量的重复计算和内存浪费。

\*\*另一个例子是结构化输出（如JSON格式）的受限解码：\*\*在这种情况下，LLM的输出需要遵循由正则表达式定义的特定语法规则。在这些规则的限制下，很多标记其实是可以一次性解码的。但遗憾的是，当前的系统却只能逐一解码标记，这无疑降低了整体的解码效率。

面对这些挑战，我们开发了SGLang，这是一种专为大型语言模型（LLMs）设计的结构化生成语言。SGLang的核心优势在于，它能够巧妙地利用语言模型（LM）程序中的多层调用结构，显著提升执行效率。

\*\*SGLang由两大核心部分组成：\*\*前端语言与后端运行时。前端部分让LM程序的编写变得更为简便快捷，而后端运行时则负责加速程序的执行过程。这两部分既可以紧密合作，共同提升整体性能，也可以各自独立运行，灵活应对不同需求。

值得一提的是，SGLang被设计为嵌入Python的特定领域语言（DSL）。它不仅提供了丰富的生成基元（如扩展、生成、选择），还支持并行控制（如分叉、合并）。由于与Python的控制流和库无缝兼容，用户只需运用熟悉的Python语法，即可轻松构建高级提示工作流程。

为了支持SGLang的高效运行，我们还特别提供了专用的解释器和编译器。解释器能够智能管理提示状态，将其视为数据流进行处理，并异步执行基元操作，确保对同步和程序内并行的精准控制。此外，用户还可以对SGLang程序进行追踪和编译，以进一步优化其性能表现。

**在运行时优化方面，我们设计了几项新颖的策略来加速SGLang程序的执行效率**。

首先，我们引入了“基数注意力”（RadixAttention）技术，这项技术允许在多次生成过程中自动复用键值（KV）缓存。传统上，许多推理引擎在完成一个请求的处理后就会丢弃其KV缓存，导致这一宝贵资源无法在多次请求间共享，从而拖慢了执行速度。而我们的系统则创新性地为所有请求在基数树结构中维护了一个基于最近最少使用（LRU）原则的KV缓存。这样，我们不仅将KV缓存管理得像传统缓存一样高效，还利用基数树实现了缓存的快速匹配、插入和替换。通过这种方式，我们的系统能够更智能地处理各种缓存复用模式，显著提升执行效率。

其次，我们采用了压缩有限状态机技术，以实现结构化输出的快速约束解码。传统方法在处理约束时，每次只关注下一个标记，通过屏蔽不允许的标记概率来实现解码，这种方法每次只能解码一个标记，效率较低。而我们的系统则能够分析约束条件，并构建一个压缩有限状态机来代表这些约束。这个状态机能够在可能的情况下将原本需要多步解码的标记路径压缩为单步路径，从而允许同时解码多个标记，极大地提升了解码速度。

最后，针对像OpenAI GPT-4这样的仅API模型，我们特别引入了“API推测执行”技术来优化多调用程序的执行。这一技术能够更高效地处理这类模型中的多调用请求，进一步提升整体执行效率。

利用SGLang，我们实现了多种大型语言模型（LLM）应用，涵盖代理控制、逻辑推理、小样本学习基准测试、JSON解码、检索增强生成流程、多轮对话以及多模态处理。我们在NVIDIA A10G和A100 GPU上测试了包括Llama-7B/70B、Mistral-8x7B、LLaVA-v1.5-7B（图像）和LLaVA-NeXT-34B（视频）在内的模型性能。实验结果表明，与现有的编程和推理系统（如Guidance、vLLM和LMQL）相比，SGLang在各种工作负载、模型和硬件配置下，吞吐量提高了高达6.4倍。

# **3 基于RadixAttention的高效键值缓存复用**

SGLang程序通过“fork”基础操作，能够串联多个生成调用，并创建并行副本。同时，不同的程序实例之间经常共享某些共通部分，比如系统提示符。这些共通部分在执行过程中会形成很多共享的提示符前缀，从而为KV缓存的重用提供了大量机会。在大型语言模型（LLM）的推理阶段，KV缓存负责存储前向传递过程中的中间张量，这些张量随后会被用于解码未来的标记。这些缓存以自注意力机制中的键值对命名。具有相同提示符前缀的请求就可以共享同一KV缓存，从而有效减少计算冗余和内存消耗。

在优化SGLang程序时，一个亟待解决的问题是如何跨越多个调用和实例来高效地重用KV缓存。尽管市面上已有一些系统尝试实现KV缓存的复用，但这些方法往往操作繁琐，需要人工配置不说，还难以应对各种复杂的复用场景，比如动态变化的树结构。正因如此，目前大多数领先的推理系统都采取了一种相对保守的做法：对每个请求都重新计算KV缓存。

RadixAttention一种革命性的新技术，它能在程序运行时自动又智能地重用那些宝贵的KV缓存。跟以前那些“用完就扔”的系统不一样，RadixAttention会把缓存像宝贝一样存起来，放在一个叫做基数树的地方，专门用来存放那些提示符和生成的结果。为了让缓存更加高效，我们还特别设计了两种策略：一种是“最近最少使用”（LRU）的淘汰策略，它会自动把最不常用的缓存清理出去，给新的内容腾地方；另一种是缓存感知的调度策略，它能更聪明地安排缓存的使用，提高缓存的命中率。

\*\*RadixAttention：\*\*基数树作为一种先进的数据结构，相较于传统的字典树（或称前缀树），显著提升了空间利用效率。它打破了常规树结构中边仅代表单个元素的限制，使得基数树的边能够表示元素的连续序列，这一特性极大地加速了数据处理速度。

在我们的系统中，基数树被巧妙地用来管理token序列与相应KV缓存张量之间的映射关系。这些KV缓存张量并不连续存储，而是分布在按页划分的内存空间中，每页大小与一个特定的标记相匹配。

鉴于GPU内存资源宝贵且易于被迅速消耗的特点，我们实施了一个简化的最近最少使用（LRU）淘汰策略，该策略优先移除那些最长时间未被访问的叶子节点。通过这一策略，我们不仅有效释放了被淘汰叶子节点所占用的内存，还充分利用了这些节点的共同祖先节点，实现内存的最大化复用。随着系统运行，当这些祖先节点也变成孤立的叶子节点并符合淘汰条件时，它们同样会被有序地逐出，从而保持整个缓存系统的健康与高效。

!\[图片\](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

在连续批处理的场景中，对于当前正在处理的批次所使用的节点，我们无法直接进行移除。为此，每个节点都配备了一个引用计数器，该计数器实时记录着有多少个运行中的请求正在使用该节点。只有当节点的引用计数器归零时，该节点才会被视为可移除的。值得注意的是，我们并没有预先设定一个固定大小的内存池作为缓存区，而是让缓存的标记与当前正在处理的请求共享同一个内存资源池。这样，系统就能根据实际需求，动态地为缓存和正在运行的请求分配内存。

当系统中积累了足够多的待处理请求时，为了支持更大的批次处理，系统会优先移除所有缓存的标记。图3展示了如何针对多个传入的请求来维护基数树。在这个过程中，前端解释器会将完整的指令发送给运行时环境，由运行时环境负责执行前缀匹配和重用操作。基数树的结构被存储在CPU上，其维护成本几乎可以忽略不计。当执行fork原语时，前端会首先发送前缀作为提示，确保该前缀能正确地被插入到树中，随后再发送剩余的指令。这种“前端提示”机制简化了运行时的调度和匹配流程，充分展示了前端与运行时协同设计的优势。

缓存感知调度：我们定义缓存命中率为缓存中已命中提示标记的数量与总提示标记数量的比值。当等待队列中存在大量请求时，它们执行的顺序会显著影响缓存命中率。例如，如果请求调度器频繁地在不同且无关的请求之间切换，这可能导致缓存颠簸和较低的命中率。为此，我们设计了一种缓存感知调度算法，以提高缓存命中率。

在批量处理设置中，我们按照匹配前缀的长度对请求进行排序，并优先处理匹配前缀更长的请求，而非遵循先到先服务的原则。在更注重延迟敏感性的场景中，我们或许仍然能够容忍有限的批次重新排序，以改善缓存复用。

此外，我们证明了在离线情况下最优调度的以下定理：

定理3.1：对于一批请求，我们可以通过以深度优先搜索顺序遍历请求的基数树，并在缓存大小大于等于最大请求长度的情况下，实现最优的缓存命中率。最长共享前缀优先顺序与深度优先搜索顺序是等价的。

RadixAttention可以扩展到多个GPU上运行。在张量并行化方面，每个GPU维护一个分片化的KV缓存。由于树操作是相同的，因此无需额外的同步。

为了适应多副本工作者（即数据并行）的分布式环境，我们对RadixAttention进行了改进，设计了一种机制：每个工作者维护自己的子树，而路由器则监督一个元树（meta-tree）。这个元树充当一个字典树（trie），用于跟踪所有子树及其关联的设备。当新一批请求到达路由器时，会在元树上执行前缀匹配。我们根据每个请求与特定工作者及同组中其他请求的共享前缀长度（即请求的亲和性）来实施各种策略，以做出高效的分发决策，从而最小化冗余计算。每次处理新请求时，路由器和工作者都会独立更新各自的树。如果工作者节点发生逐出操作，它会将此操作提交到一个队列中，路由器在低活动期间处理此队列以更新元树。

我们使用四个工作者和MMLU数据集对这种分布式配置进行了基准测试，观察到它实现了线性扩展和最优缓存命中率，同时这种弱一致性分布式缓存设计带来的开销极低。在最大化数据局部性和并行处理效率之间存在权衡。探索高级调度策略以优化这一权衡被指定为未来研究的领域。此外，Preble 的并发工作基于SGLang的早期版本研究了数据并行调度。

# **4 高效的受限解码：压缩有限状态机**

在语言模型（LM）的应用中，用户常常希望模型的输出能够严格遵循特定的格式，比如JSON结构，这样做不仅增强了输出的可控性和鲁棒性，还使得后续处理更加便捷。SGLang为满足这一需求，引入了正则表达式参数作为约束工具，通过将其转换为有限状态机（FSM）来确保输出格式的正确性。这一方法在实践中已被证明相当有效。

!\[图片\](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

在解码过程中，它们保持当前的有限状态机（FSM）状态，从后续状态中检索允许的标记，并将无效标记的概率设置为零，逐个标记进行解码。然而，当有机会一次性解码多个标记时，这种逐个标记的方法效率不高。例如，在图4（c）所示的正常解码过程中，常序列“summary:”跨越了多个标记，尽管在解码时只有一个有效的后续标记，但仍需要多个解码阶段。因此，整个序列原本可以在单一步骤（即前向传递）中完成解码。然而，由于现有系统中有限状态机（FSM）与模型运行器之间缺乏集成，导致无法处理多标记，因此现有系统只能一次解码一个标记，从而造成了解码速度缓慢。

为了克服这一限制，SGLang设计了一个创新的解决方案——快速受限解码运行时，该运行时通过压缩FSM来优化解码过程。具体来说，它会对FSM进行分析，将原本可能存在的多个连续且单一转换的边合并成一条边，如图4（b）所示。这种压缩机制使得系统能够智能地识别出哪些标记组合可以一次性解码，从而避免了不必要的多次解码过程。在图4（d）的示例中，我们可以看到，通过压缩转换边，原本需要分步解码的多个标记现在可以在一次前向传递中完成，极大地提升了解码速度。这一方法不仅适用于图示的特定情况，而且具有通用性，能够应用于所有基于正则表达式的输出约束。

## **4.1 压缩有限状态机**

我们的目标是让大型语言模型（LLMs）遵循正则表达式（regex），这种表达式具有更高的表达能力，可用于表示诸如JSON模式等常见格式。为实现这一目标，我们将正则表达式转换为有限状态机（FSM），以在解码过程中指导生成流程。FSM本质上是一个由节点（状态）和边（带字符串/字符的转换）构成的图。从初始状态开始，每次转换都会将边上的字符串附加到当前状态，以移至下一个状态，最终通过一组终态来结束整个过程。这一机制指导LLMs的解码过程，根据FSM的当前状态转换过滤掉无效的标记，如图10所示。解码过程中，FSM可能会经历多次状态转换，直至达到终态。

!\[图片\](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

受限解码的挑战源自这样一个事实：约束条件通常以自然语言格式表达，即正则表达式通过字符/字符串来描绘，而大型语言模型（LLMs）则被设计为将这些内容解读并处理为标记（tokens）。这导致了一个复杂的场景，因为字符串与标记之间的映射关系错综复杂，且缺乏一一对应的关系。

\*\*压缩有限状态机（FSM）的实现细节：\*\*为了简化压缩有限状态机（FSM）的构建过程，我们基于字符/字符串而非标记来构建原始FSM。在此，我们正式定义单一转换边和压缩边的概念如下：

• 单一转换边：如果一条边满足以下条件，则称其为单一转换边：1) 其源节点仅有一个后继节点；2) 该边上仅有一个可接受的字符/字符串。

• 压缩边：若连续相邻的边（e0, e1, ..., ek）均可被视为单一转换边，则这些边可合并压缩成一条边。压缩边的文本是e0, e1, ..., ek各边文本的连接。

!\[图片\](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

从基于字符的FSM出发，我们递归地将单一转换边合并到其前面的边上，直至无法进一步压缩，从而得到压缩FSM。此方法通过提升解码过程的速度证明了其有效性，如图11所示，SGLang在采用压缩FSM后展现了高效的运行时性能。

# **5 高效的LLM调用：API预测执行**

在前面的章节里，我们已经探讨了如何优化开放权重模型，这些优化涉及对模型推理流程的精细调整。此外，SGLang 还适用于仅通过 API 访问的模型，例如 OpenAI 的 GPT-4。然而，对于这些模型，我们只能调用一个黑箱 API 端点。

针对这一挑战，我们开发了一种新颖的优化策略，专门用于减少调用这些黑箱API模型时的成本和延迟。以具体场景为例，当使用SGLang编写程序要求模型生成一个角色的详细信息时，程序可能会连续两次调用API：第一次生成角色的名字，第二次生成其职业。这种情况下，传统的做法意味着用户需要为每次API调用支付费用，即使这些调用本质上都基于同一上下文。

为了优化这一过程，我们在SGLang中引入了预测执行的概念。简单来说，就是在第一次API调用时，不是立即停止生成，而是让模型继续“预测性”地多生成一些内容。这些额外生成的内容会被解释器保存下来，并尝试与后续的操作需求（如生成职业）进行匹配和重用。通过精心设计的提示（即“提示工程”），我们有可能让模型以相当高的准确率直接生成出符合后续模板需要的内容，从而省去了另一次API调用的必要，同时也降低了总体延迟并节省了成本。

**6 相关工作**

众多研究已经深入探讨了键值（KV）缓存的重用策略，其中不少与我们的工作并驾齐驱。而我们的RadixAttention方法独树一帜，首次提出将KV缓存视为基于树的最近最少使用（LRU）缓存机制。这一创新不仅支持多级共享、缓存感知调度、前端与运行时协同调度，还能应对分布式场景。

相比之下，vLLM\[和ChunkedAttention虽然探索了简单的重用案例，如系统提示共享，但并未触及多级树形结构共享或LRU缓存的层面。

PromptCache虽然提出了前缀之外的KV缓存模块化重用方案，但可能导致高达43%的精度损失。HydraGen、FlashInfer和ChunkedAttention则专注于CUDA内核优化，忽略了LRU缓存的重要性。

API Serve和LLM-SQL则针对特定应用（如与外部API调用和关系数据库的结合）研究了KV缓存的重用，但并未采用我们的基数树或缓存感知调度技术。

在大型语言模型（LLM）编程和代理框架领域，Guidance、LMQL、DSPy、LangChain、AutoGen和LLM Compiler等已有所建树。其中，Guidance和LMQL与我们的SGLang框架最为相似。

SGLang的创新之处在于其独特的运行时优化技术，专为加速编程模型而设计。同时，SGLang还具备与其他框架的兼容性，并能通过优化加速它们的运行（例如，我们在评估中展示了如何加速DSPy）。此外，SGLang还兼容多种常见的推理优化方法，进一步提升了其在实际应用中的灵活性和效率。

**7 总结**

\*\*未来展望：\*\*尽管SGLang已经取得了显著成就，但仍存在一些局限性，这些局限性为未来的研究指明了充满潜力的方向。

扩展SGLang的功能，以支持更多样化的输出模式；调整RadixAttention算法，使其能够跨越不同的内存层次（如DRAM和磁盘）高效工作；

在RadixAttention中引入模糊语义匹配功能；在SGLang的基础上提供更高层次的编程原语；解决缓存感知调度中的资源饥饿问题；并进一步增强SGLang编译器，以执行包括调度和内存规划在内的高级静态优化。

\*\*总结：\*\*我们推出了SGLang框架，它专为高效编程和执行结构化语言模型程序而设计。通过RadixAttention、压缩有限状态机以及强大的语言解释器等创新优化技术，SGLang极大地提升了复杂语言模型程序的吞吐量和响应速度。它是研究和开发高级提示技术及智能代理工作流的强大工具。

现在，SGLang的源代码已全面开放，欢迎广大开发者共同探索其无限可能。

!\[图片\](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

!\[图片\](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

![](https://mmbiz.qlogo.cn/mmbiz_jpg/iaicHNqyB8TgqKNAzs6pYOibDas08icibS0nc5HHwTKEq9fLEfKxoja8sPTvPjEnbBDRsOGouJhFE4MZ9ibh5sw7RkiaQ/0?wx_fmt=jpeg)

MLSys2024

![赞赏二维码](https://mp.weixin.qq.com/s?__biz=MzA5MTQxMTM0Nw==&mid=2247485095&idx=1&sn=077f0c684f7a410b50820c7fd6e99223&chksm=907d805ba70a094df8268543cb22cab4d40ba2bc467e9496ca2570bcc7f2e24d0d6e51db7375&mpshare=1&scene=24&srcid=0731JoOb6TqoNs7t9kogDFcz&sharer_shareinfo=d3ce57db4b10b6e6ba1023c3867e8d0e&sharer_shareinfo_first=d3ce57db4b10b6e6ba1023c3867e8d0e&key=daf9bdc5abc4e8d0657cc3ecfbac014bf1eaef6c1e6b522afbfc18eb3081adf9c755462ae5d24af9cfcfe70cc22c9190afc14b08e152d8758846e0c10d54e951a1ea84d53007c9e82a42343be7924dc8c8de3d5817ad4732be9f697621afddfba03bed2c6e2d2c44b264dc50c913b293324c911ccbd014a411289fb064ae3f21&ascene=0&uin=MTEwNTU1MjgwMw%3D%3D&devicetype=Windows+11+x64&version=63090621&lang=zh_CN&countrycode=CN&exportkey=n_ChQIAhIQzDAyt%2BGyPo854k5LWTb15xLmAQIE97dBBAEAAAAAAO7HC7RuUvkAAAAOpnltbLcz9gKNyK89dVj0mZe233fZ2f32U3JVRrkk5NkohD9ZIf8RVE0I7V1esMTHxkUmY4HqfMYYoHptJffiBPsGTUagjOskAsUbzg8CLVsn8I2mNeaO0OWXQR6fNDT2xbCwDW9QZs%2FoD0SLQhmYFRj4oCKkJF6mGp2nzA065wvfzv6SufjBWeJcR8M4g0AMz4DhEukJmNSG3Cu0fSbqAe6JhoNgI1XL0G8uM6DHRHFgCD%2Fin%2FgJiDPR0pTUZju4z2erRQy%2FffV0Kdx8V6L6&acctmode=0&pass_ticket=%2FqLPczAfnEyFsYq6vY%2BOoYni0b4fX2mHjudr4cAnBrscTTGP3qT66pY0xsXb2r3r&wx_header=1&fasttmpl_type=0&fasttmpl_fullversion=7350504-zh_CN-zip&fasttmpl_flag=1)喜欢作者

大模型推理23

大模型推理 · 目录

上一篇DistriFusion：CVPR 2024 重点推荐，扩散模型推理6倍加速，高清图像无损生成下一篇LLM 20,000 QPS背后的力量：揭秘Character AI的高效推理技术

阅读 2877

​

写留言

[](javacript:;)

![](http://mmbiz.qpic.cn/sz_mmbiz_png/iakNhheymmzB3IYQOVY15KLoskTXtboZ5lxM2eTLCQc4y94Lnj86wqPGMzuvVoxsL2oOp09FJFkOCdH8nEhLDeg/300?wx_fmt=png&wxfrom=18)

MLSys2024

283017

写留言

写留言

**留言**

暂无留言
